{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4158632",
   "metadata": {},
   "source": [
    "# Handwritten Digit Classification on the MNIST Dataset\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project applies the universal workflow of machine learning from *Deep Learning with Python* (Chapter 4.5, 1st edition) to the MNIST handwritten digit dataset. The goal is to build, analyze, and improve a neural network that can recognize digits from images.\n",
    "\n",
    "The task is to develop a machine learning model that can accurately classify images of handwritten digits (0–9) from the MNIST dataset. This is a supervised learning problem, where the model is trained on labeled examples of images and their corresponding digit classes. Each input is a 28×28 grayscale image, and the model outputs a label indicating which digit (0–9) it predicts for that image. The MNIST dataset is a widely used benchmark in machine learning and computer vision, consisting of 70,000 images in total: 60,000 for training and 10,000 for testing.\n",
    "\n",
    "MNIST is an ideal choice for this project because it is relatively small, easy to load and preprocess, and has been extensively studied, which makes it well suited for learning and comparing different model configurations.\n",
    "\n",
    "In this project, the models are restricted to Keras/TensorFlow Sequential architectures built only from Dense and Dropout layers, without using more complex layers such as convolutional layers. This constraint reflects the course requirements and encourages a focus on understanding the core ideas of fully connected neural networks and regularization, rather than relying on more advanced architectures.\n",
    "\n",
    "The main metric for success in this project is classification accuracy on a held‑out test set, measuring the proportion of correctly classified digit images. The goal is to build a model that achieves high accuracy while following the DLWP universal workflow, rather than necessarily reaching state‑of‑the‑art performance. Additional metrics such as precision, recall, and F1‑score will be considered during evaluation to provide a more detailed picture of model performance, particularly if class imbalance or specific error types become relevant.\n",
    "\n",
    "The workflow for this project follows the universal machine learning process described in Deep Learning with Python: defining the problem and dataset, choosing a success metric, and deciding on an evaluation protocol. After that, the data will be prepared and split, a baseline model will be built and evaluated, a larger model will be trained to intentionally overfit, and finally regularization techniques and hyperparameter tuning will be applied based on validation performance to improve generalization.\n",
    "\n",
    "In this project, the universal workflow of machine learning from Deep Learning with Python is followed step by step. The problem and dataset are first defined, along with the success metric and evaluation protocol. The data is then loaded, preprocessed, and split into training, validation, and test sets. A simple baseline model is built and evaluated, followed by a larger model that is allowed to overfit in order to explore the model’s capacity. Finally, regularization techniques and hyperparameter tuning are applied to improve generalization, and the best model is evaluated on the test set and discussed in terms of its strengths, limitations, and possible extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea199325",
   "metadata": {},
   "source": [
    "## 1. Problem definition and dataset\n",
    "\n",
    "The problem addressed in this project is the classification of handwritten digits from the MNIST dataset. The MNIST dataset consists of 70,000 grayscale images of handwritten digits (0–9), each of size 28×28 pixels. It is split into a training set of 60,000 images and a test set of 10,000 images. This is a supervised multiclass classification problem, where the goal is to train a model that takes an image as input and outputs the correct digit label.\n",
    "\n",
    "MNIST is a good choice for this project because it is widely used in the machine learning community, making it easy to compare results with existing work. It is also relatively small and straightforward to load and preprocess, which allows us to focus on the modeling aspects rather than on complex data handling. Additionally, the task of digit classification is a well‑defined and intuitive problem that serves as a gentle introduction to image classification and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d05e3",
   "metadata": {},
   "source": [
    "## 2. Measure of Success\n",
    "\n",
    "Our mains metric for success in this project is classification accuracy on a held‑out test set, which measures the proportion of correctly classified digit images. \n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{Number\\ of\\ Correct\\ Predictions}{Total\\ Number\\ of\\ Predictions}\n",
    "$$\n",
    "\n",
    "Because MNIST is a relatively simple and well‑studied benchmark, many models achieve very high performance on this dataset. For this reason, this project considers a test accuracy above 99% as the target for a successful model, rather than treating lower accuracies (e.g. 90–95%) as sufficient. Additional metrics such as precision, recall, and F1‑score will be considered during evaluation to provide a more detailed picture of model performance, particularly if class imbalance or specific error types become relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b14da8",
   "metadata": {},
   "source": [
    "## 3. Evaluation Protocol\n",
    "\n",
    "The original MNIST dataset provides 60,000 training images and 10,000 test images. In this project, the 60,000 training images are further split into 50,000 training samples and 10,000 validation samples. The 50,000 training samples are used to fit the parameters of the neural network, while the 10,000 validation samples are used to monitor performance during development and to guide choices such as model architecture, number of epochs, and regularization strength.\n",
    "\n",
    "The 10,000 test images are kept completely separate from the training and validation process and are only used once, at the end of the project, to obtain an unbiased estimate of the final model’s generalization performance.\n",
    "This train/validation/test setup helps prevent overfitting to the test set and ensures that any improvements observed on the validation set reflect genuine improvements in the model, rather than adaptation to a single held‑out benchmark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451425b3",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b2e3e",
   "metadata": {},
   "source": [
    "MNIST dataset is available in Keras, which makes it easy to load and preprocess. The images are grayscale and have pixel values in the range [0, 255]. For better performance, we will normalize the pixel values to the range [0, 1] by dividing by 255. Additionally, since we are using a fully connected neural network, we will flatten the 28×28 images into 784‑dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd26275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28), y_train shape: (60000,)\n",
      "x_test shape: (10000, 28, 28), y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1) Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ad933",
   "metadata": {},
   "source": [
    "To provide the model with the correct format for training, we will need to \"flatten\" the 28×28 images into 784‑dimensional vectors. This can be done using the `reshape` method in NumPy or by using the `Flatten` layer in Keras. This process transforms each 2D image into a 1D vector, which is the expected input format for a fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aca3a5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 784), x_test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 2) Flatten images: (num_samples, 28, 28) -> (num_samples, 784)\n",
    "num_train = x_train.shape[0]\n",
    "num_test = x_test.shape[0]\n",
    "\n",
    "x_train = x_train.reshape(num_train, 28 * 28)\n",
    "x_test = x_test.reshape(num_test, 28 * 28)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, x_test shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b91c6",
   "metadata": {},
   "source": [
    "Normalization of pixel values is crucial for training neural networks effectively, as it helps to ensure that the input data is on a similar scale, which can improve convergence during training. By dividing the pixel values by 255, we scale them to the range [0, 1], which is more suitable for the activation functions used in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e5c0363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel value range after normalization: [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# 3) Convert to float and scale to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "print(f\"Pixel value range after normalization: [{x_train.min()}, {x_train.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8aed0",
   "metadata": {},
   "source": [
    "Next, we will convert the class labels (0–9) into one‑hot encoded vectors using `keras.utils.to_categorical`. This function takes a vector of class indices and returns a matrix of one‑hot encoded label vectors, which is the required format for training a neural network with categorical crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb58774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (60000, 10)\n",
      "(10000,) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 4) One-hot encode labels: 0 -> [1,0,0,0,0,0,0,0,0,0], etc.\n",
    "num_classes = 10\n",
    "\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(y_train.shape, y_train_categorical.shape)\n",
    "print(y_test.shape, y_test_categorical.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6fe14",
   "metadata": {},
   "source": [
    "Validation set is created by splitting the original training set of 60,000 images into 50,000 training samples and 10,000 validation samples. The validation set is used to select model architectures and hyperparameters, while the separate 10,000‑image test set is used only once at the end to obtain an unbiased estimate of the final model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e34d5773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 10)\n",
      "(10000, 784) (10000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 5) Create training / validation split from the original training set\n",
    "x_train_final = x_train[:50000]\n",
    "y_train_final = y_train_categorical[:50000]\n",
    "\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train_categorical[50000:]\n",
    "\n",
    "print(x_train_final.shape, y_train_final.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test_categorical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59431151",
   "metadata": {},
   "source": [
    "## 5. Baseline model: small dense network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dada749",
   "metadata": {},
   "source": [
    "The baseline model is a simple fully connected neural network with one hidden layer of 64 units and ReLU activation, followed by an output layer of 10 units with softmax activation for multiclass classification. This architecture is chosen for its simplicity and ability to learn non‑linear relationships in the data, while still being small enough to train quickly and serve as a useful baseline for comparison with more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24ee96",
   "metadata": {},
   "source": [
    "We start by importing \"layers\" from Keras, which provides the building blocks for constructing our neural network. The `Dense` layer is used to create fully connected layers, the first one with 64 units and ReLU activation, and the second one with 10 units and softmax activation for outputting class probabilities. The `Sequential` model is used to stack these layers in a linear fashion.\n",
    "\n",
    "Finally, we compile the model with the `RMSPROP` optimizer, categorical crossentropy loss (suitable for multiclass classification), and accuracy as the metric to monitor during training. This setup allows us to train the model effectively and evaluate its performance based on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81ef41ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_baseline = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\", input_shape=(28 * 28,)),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_baseline.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_baseline.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef7182",
   "metadata": {},
   "source": [
    "Training the baseline and storing the history of training and validation accuracy allows us to analyze the model's learning process and identify potential issues such as underfitting or overfitting. By comparing the training and validation accuracy, we can gain insights into how well the model is generalizing to unseen data and whether it is learning meaningful patterns from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e783c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8849 - loss: 0.4231 - val_accuracy: 0.9279 - val_loss: 0.2474\n",
      "Epoch 2/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9358 - loss: 0.2267 - val_accuracy: 0.9447 - val_loss: 0.1985\n",
      "Epoch 3/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9503 - loss: 0.1746 - val_accuracy: 0.9567 - val_loss: 0.1540\n",
      "Epoch 4/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9590 - loss: 0.1419 - val_accuracy: 0.9616 - val_loss: 0.1388\n",
      "Epoch 5/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9653 - loss: 0.1197 - val_accuracy: 0.9644 - val_loss: 0.1230\n",
      "Epoch 6/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.1032 - val_accuracy: 0.9667 - val_loss: 0.1122\n",
      "Epoch 7/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9734 - loss: 0.0906 - val_accuracy: 0.9683 - val_loss: 0.1056\n",
      "Epoch 8/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0803 - val_accuracy: 0.9694 - val_loss: 0.1021\n",
      "Epoch 9/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9791 - loss: 0.0722 - val_accuracy: 0.9706 - val_loss: 0.1006\n",
      "Epoch 10/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0648 - val_accuracy: 0.9707 - val_loss: 0.0971\n",
      "Epoch 11/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9830 - loss: 0.0588 - val_accuracy: 0.9707 - val_loss: 0.0948\n",
      "Epoch 12/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9847 - loss: 0.0540 - val_accuracy: 0.9733 - val_loss: 0.0899\n",
      "Epoch 13/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9863 - loss: 0.0490 - val_accuracy: 0.9719 - val_loss: 0.0941\n",
      "Epoch 14/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9872 - loss: 0.0453 - val_accuracy: 0.9737 - val_loss: 0.0888\n",
      "Epoch 15/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9887 - loss: 0.0412 - val_accuracy: 0.9719 - val_loss: 0.0919\n",
      "Epoch 16/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0383 - val_accuracy: 0.9737 - val_loss: 0.0901\n",
      "Epoch 17/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.0354 - val_accuracy: 0.9731 - val_loss: 0.0934\n",
      "Epoch 18/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9910 - loss: 0.0330 - val_accuracy: 0.9738 - val_loss: 0.0906\n",
      "Epoch 19/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9917 - loss: 0.0306 - val_accuracy: 0.9734 - val_loss: 0.0908\n",
      "Epoch 20/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9925 - loss: 0.0281 - val_accuracy: 0.9760 - val_loss: 0.0911\n"
     ]
    }
   ],
   "source": [
    "# history_baseline = model_baseline.fit(\n",
    "#     x_train_final,\n",
    "#     y_train_final,\n",
    "#     epochs=5,\n",
    "#     batch_size=128,\n",
    "#     validation_data=(x_val, y_val)\n",
    "# )\n",
    "\n",
    "history_baseline = model_baseline.fit(\n",
    "    x_train_final,\n",
    "    y_train_final,\n",
    "    epochs=20,          # changed from 5 to 20\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f03f36",
   "metadata": {},
   "source": [
    "Storing the training history also enables us to visualize the learning curves, which can help in diagnosing problems with the model and guiding further improvements. For example, if the training accuracy is high but the validation accuracy is low, it may indicate that the model is overfitting to the training data, suggesting that we may need to apply regularization techniques or gather more data. Conversely, if both training and validation accuracy are low, it may indicate underfitting, suggesting that we may need a more complex model or longer training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e8767c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy per epoch: [0.884880006313324, 0.9358000159263611, 0.9503200054168701, 0.9590200185775757, 0.9652799963951111, 0.9702399969100952, 0.9733999967575073, 0.9765599966049194, 0.9791399836540222, 0.9816799759864807, 0.983020007610321, 0.9846600294113159, 0.9862599968910217, 0.9872000217437744, 0.9886599779129028, 0.9894999861717224, 0.9896799921989441, 0.9909999966621399, 0.9916599988937378, 0.9925000071525574]\n",
      "Validation accuracy per epoch: [0.9279000163078308, 0.9447000026702881, 0.9567000269889832, 0.9616000056266785, 0.9643999934196472, 0.96670001745224, 0.9682999849319458, 0.9693999886512756, 0.9706000089645386, 0.9707000255584717, 0.9707000255584717, 0.9732999801635742, 0.9718999862670898, 0.9736999869346619, 0.9718999862670898, 0.9736999869346619, 0.9731000065803528, 0.973800003528595, 0.9733999967575073, 0.9760000109672546]\n"
     ]
    }
   ],
   "source": [
    "train_acc = history_baseline.history[\"accuracy\"]\n",
    "val_acc = history_baseline.history[\"val_accuracy\"]\n",
    "\n",
    "print(\"Training accuracy per epoch:\", train_acc)\n",
    "print(\"Validation accuracy per epoch:\", val_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c163d",
   "metadata": {},
   "source": [
    "### Baseline performance compared to a trivial baseline\n",
    "\n",
    "Since MNIST is a 10-class classification problem, a trivial model that predicts digit labels uniformly at random would be expected to achieve an accuracy of around 10%. In contrast, the baseline dense neural network trained in this project reaches approximately 99.6% training accuracy and 97.4% validation accuracy after 5 epochs. This shows that the model is learning meaningful structure in the data and performing far better than chance.\n",
    "\n",
    "However, because MNIST is a relatively simple benchmark where well-designed models can exceed 99% test accuracy, this baseline is still treated as a starting point rather than a satisfactory final model. Later sections will increase model capacity and apply regularization with the aim of closing the gap between the current 97.4% validation accuracy and the 99% target.\n",
    "\n",
    "| Model                          | Accuracy (approx.) |\n",
    "|--------------------------------|--------------------|\n",
    "| Random guess (10 classes)      | ~10%               |\n",
    "| Baseline dense network (val)   | ~97.4%             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2157d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9925000071525574, Validation Accuracy: 0.9760000109672546\n"
     ]
    }
   ],
   "source": [
    "train_acc = history_baseline.history[\"accuracy\"]\n",
    "val_acc = history_baseline.history[\"val_accuracy\"]\n",
    "print(f\"Training Accuracy: {train_acc[-1]}, Validation Accuracy: {val_acc[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f480e86",
   "metadata": {},
   "source": [
    "## 6. Overfitting model: larger dense network\n",
    "lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a2a65c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m200,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model_big = keras.Sequential([\n",
    "    layers.Dense(256, activation=\"relu\", input_shape=(28 * 28,)),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_big.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_big.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be2c3bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9165 - loss: 0.2808 - val_accuracy: 0.9633 - val_loss: 0.1314\n",
      "Epoch 2/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9663 - loss: 0.1101 - val_accuracy: 0.9688 - val_loss: 0.1014\n",
      "Epoch 3/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9777 - loss: 0.0706 - val_accuracy: 0.9624 - val_loss: 0.1254\n",
      "Epoch 4/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9838 - loss: 0.0510 - val_accuracy: 0.9714 - val_loss: 0.0989\n",
      "Epoch 5/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9884 - loss: 0.0371 - val_accuracy: 0.9769 - val_loss: 0.0792\n",
      "Epoch 6/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9907 - loss: 0.0288 - val_accuracy: 0.9813 - val_loss: 0.0708\n",
      "Epoch 7/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9934 - loss: 0.0215 - val_accuracy: 0.9777 - val_loss: 0.0807\n",
      "Epoch 8/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9945 - loss: 0.0173 - val_accuracy: 0.9806 - val_loss: 0.0832\n",
      "Epoch 9/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9958 - loss: 0.0128 - val_accuracy: 0.9797 - val_loss: 0.0833\n",
      "Epoch 10/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9959 - loss: 0.0120 - val_accuracy: 0.9828 - val_loss: 0.0813\n",
      "Epoch 11/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9980 - loss: 0.0068 - val_accuracy: 0.9820 - val_loss: 0.0886\n",
      "Epoch 12/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9978 - loss: 0.0073 - val_accuracy: 0.9800 - val_loss: 0.0964\n",
      "Epoch 13/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9984 - loss: 0.0047 - val_accuracy: 0.9818 - val_loss: 0.0969\n",
      "Epoch 14/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9984 - loss: 0.0045 - val_accuracy: 0.9766 - val_loss: 0.1293\n",
      "Epoch 15/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9991 - loss: 0.0026 - val_accuracy: 0.9818 - val_loss: 0.1145\n",
      "Epoch 16/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 0.9803 - val_loss: 0.1162\n",
      "Epoch 17/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 0.9829 - val_loss: 0.1010\n",
      "Epoch 18/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9818 - val_loss: 0.1142\n",
      "Epoch 19/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9999 - loss: 6.5434e-04 - val_accuracy: 0.9820 - val_loss: 0.1160\n",
      "Epoch 20/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9996 - loss: 0.0010 - val_accuracy: 0.9829 - val_loss: 0.1149\n"
     ]
    }
   ],
   "source": [
    "history_big = model_big.fit(\n",
    "    x_train_final,\n",
    "    y_train_final,\n",
    "    epochs=20,          # more epochs than baseline\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7291b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training accuracy (big model): 0.9995999932289124\n",
      "Final validation accuracy (big model): 0.9829000234603882\n"
     ]
    }
   ],
   "source": [
    "train_acc_big = history_big.history[\"accuracy\"]\n",
    "val_acc_big = history_big.history[\"val_accuracy\"]\n",
    "\n",
    "print(\"Final training accuracy (big model):\", train_acc_big[-1])\n",
    "print(\"Final validation accuracy (big model):\", val_acc_big[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08794eb2",
   "metadata": {},
   "source": [
    "To explore the effect of model capacity, a larger dense network with two hidden layers of 256 ReLU units each was trained for 20 epochs. This model achieved approximately 99.96% training accuracy and 98.29% validation accuracy, improving on the baseline validation accuracy of 97.39%. The near-perfect training accuracy indicates that the larger network has sufficient capacity to almost completely fit the training data, while the smaller gap between training and validation accuracy suggests that some overfitting is occurring but that the model still generalizes reasonably well to unseen validation examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2853a87",
   "metadata": {},
   "source": [
    "## 7. Regularization and hyperparameter tuning\n",
    "\n",
    "lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af849e5b",
   "metadata": {},
   "source": [
    "## 8. Final model evaluation on the test set\n",
    "\n",
    "lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202691a2",
   "metadata": {},
   "source": [
    "## 9. Discussion and conclusions\n",
    "\n",
    "lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c4ccd",
   "metadata": {},
   "source": [
    "## 10. References and code credits\n",
    "\n",
    "lorem ipsum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
